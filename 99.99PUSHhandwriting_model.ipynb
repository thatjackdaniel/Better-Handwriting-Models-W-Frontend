{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c3c3d6-62cc-4e75-8251-2f42306a57f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\mnist_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\ProgramData\\anaconda3\\envs\\mnist_env\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 153ms/step - accuracy: 0.4889 - loss: 1.9805 - val_accuracy: 0.9674 - val_loss: 0.3951 - learning_rate: 0.0010\n",
      "Epoch 2/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 169ms/step - accuracy: 0.8682 - loss: 0.7379 - val_accuracy: 0.9764 - val_loss: 0.4079 - learning_rate: 0.0010\n",
      "Epoch 3/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9044 - loss: 0.6582  \n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 162ms/step - accuracy: 0.9044 - loss: 0.6582 - val_accuracy: 0.9660 - val_loss: 0.5023 - learning_rate: 0.0010\n",
      "Epoch 4/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 156ms/step - accuracy: 0.9353 - loss: 0.5683 - val_accuracy: 0.9852 - val_loss: 0.3384 - learning_rate: 5.0000e-04\n",
      "Epoch 5/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 155ms/step - accuracy: 0.9476 - loss: 0.4671 - val_accuracy: 0.9872 - val_loss: 0.3082 - learning_rate: 5.0000e-04\n",
      "Epoch 6/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 156ms/step - accuracy: 0.9491 - loss: 0.4355 - val_accuracy: 0.9900 - val_loss: 0.2929 - learning_rate: 5.0000e-04\n",
      "Epoch 7/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 158ms/step - accuracy: 0.9517 - loss: 0.4206 - val_accuracy: 0.9891 - val_loss: 0.2882 - learning_rate: 5.0000e-04\n",
      "Epoch 8/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 170ms/step - accuracy: 0.9554 - loss: 0.4090 - val_accuracy: 0.9899 - val_loss: 0.2792 - learning_rate: 5.0000e-04\n",
      "Epoch 9/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 161ms/step - accuracy: 0.9565 - loss: 0.3931 - val_accuracy: 0.9857 - val_loss: 0.2844 - learning_rate: 5.0000e-04\n",
      "Epoch 10/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 173ms/step - accuracy: 0.9575 - loss: 0.3795 - val_accuracy: 0.9901 - val_loss: 0.2726 - learning_rate: 5.0000e-04\n",
      "Epoch 11/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 167ms/step - accuracy: 0.9596 - loss: 0.3795 - val_accuracy: 0.9892 - val_loss: 0.2683 - learning_rate: 5.0000e-04\n",
      "Epoch 12/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 164ms/step - accuracy: 0.9615 - loss: 0.3616 - val_accuracy: 0.9910 - val_loss: 0.2609 - learning_rate: 5.0000e-04\n",
      "Epoch 13/13\n",
      "\u001b[1m938/938\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 169ms/step - accuracy: 0.9610 - loss: 0.3576 - val_accuracy: 0.9923 - val_loss: 0.2561 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Final Loss: 0.25611, Accuracy: 99.23%\n",
      "Model saved as 'model.h5'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Model Parameters\n",
    "pixel_width = 28\n",
    "pixel_height = 28\n",
    "input_shape = (pixel_width, pixel_height, 1)\n",
    "num_of_classes = 10\n",
    "batch_size = 64  # Reasonable batch size for stability\n",
    "epochs = 13  # Keeping epochs smaller for quicker training\n",
    "learning_rate = 0.001  # Base learning rate\n",
    "dropout_rate = 0.4  # Adjusted dropout rate to prevent overfitting\n",
    "\n",
    "# Load MNIST Dataset\n",
    "(features_train, labels_train), (features_test, labels_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and Normalize Data\n",
    "features_train = features_train.reshape(-1, pixel_width, pixel_height, 1).astype(\"float32\") / 255.0\n",
    "features_test = features_test.reshape(-1, pixel_width, pixel_height, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_train = keras.utils.to_categorical(labels_train, num_of_classes)\n",
    "labels_test = keras.utils.to_categorical(labels_test, num_of_classes)\n",
    "\n",
    "# ğŸ† **Data Augmentation**: Slightly increased range for robustness\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # Increase rotation range for more variability\n",
    "    zoom_range=0.25,  # Slightly higher zoom range\n",
    "    width_shift_range=0.2,  # Horizontal shift\n",
    "    height_shift_range=0.2,  # Vertical shift\n",
    "    shear_range=0.15,  # Shear transformation\n",
    ")\n",
    "datagen.fit(features_train)\n",
    "\n",
    "# ğŸ§  **Deep CNN Model with Enhanced Regularization**\n",
    "model = Sequential()\n",
    "\n",
    "# First Conv Block\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Second Conv Block\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.0005)))\n",
    "model.add(Dropout(dropout_rate))  # Apply dropout in the FC layer\n",
    "model.add(Dense(num_of_classes, activation='softmax'))\n",
    "\n",
    "# **Optimizer with Learning Rate Decay**\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# ğŸ§  **Advanced Callbacks**: Patience for early stopping and reduce LR\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# ğŸš€ **Train Model with Augmented Data**\n",
    "model.fit(datagen.flow(features_train, labels_train, batch_size=batch_size),\n",
    "          validation_data=(features_test, labels_test),\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# ğŸ¯ **Evaluate Model**\n",
    "score = model.evaluate(features_test, labels_test, verbose=0)\n",
    "print(f\"ğŸ”¥ Final Loss: {score[0]:.5f}, Accuracy: {score[1] * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('model.h5')  # Save model to a file\n",
    "print(\"Model saved as 'model.h5'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096d60a-561b-492b-b64d-d76ba2c5a367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mnist_env]",
   "language": "python",
   "name": "conda-env-mnist_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
