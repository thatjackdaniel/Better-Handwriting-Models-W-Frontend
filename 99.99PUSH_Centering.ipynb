{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c3c3d6-62cc-4e75-8251-2f42306a57f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 181ms/step - accuracy: 0.5395 - loss: 1.5329 - val_accuracy: 0.9636 - val_loss: 0.1993 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 176ms/step - accuracy: 0.8374 - loss: 0.6024 - val_accuracy: 0.9516 - val_loss: 0.2454 - learning_rate: 9.9606e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 175ms/step - accuracy: 0.8734 - loss: 0.5052 - val_accuracy: 0.9760 - val_loss: 0.1801 - learning_rate: 9.8041e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 189ms/step - accuracy: 0.8912 - loss: 0.4529 - val_accuracy: 0.9779 - val_loss: 0.1749 - learning_rate: 9.4599e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 184ms/step - accuracy: 0.9005 - loss: 0.4271 - val_accuracy: 0.9795 - val_loss: 0.1769 - learning_rate: 8.8749e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 194ms/step - accuracy: 0.9120 - loss: 0.4021 - val_accuracy: 0.9629 - val_loss: 0.2324 - learning_rate: 8.0275e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9182 - loss: 0.3801  \n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.000346988788805902.\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 190ms/step - accuracy: 0.9182 - loss: 0.3801 - val_accuracy: 0.9021 - val_loss: 0.3762 - learning_rate: 3.4699e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 177ms/step - accuracy: 0.9232 - loss: 0.3550 - val_accuracy: 0.9823 - val_loss: 0.1530 - learning_rate: 2.8409e-04\n",
      "Epoch 9/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 189ms/step - accuracy: 0.9310 - loss: 0.3168 - val_accuracy: 0.9717 - val_loss: 0.1797 - learning_rate: 2.1816e-04\n",
      "Epoch 10/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 179ms/step - accuracy: 0.9333 - loss: 0.3063 - val_accuracy: 0.9786 - val_loss: 0.1553 - learning_rate: 1.5553e-04\n",
      "Epoch 11/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 189ms/step - accuracy: 0.9355 - loss: 0.2945 - val_accuracy: 0.9782 - val_loss: 0.1505 - learning_rate: 1.0180e-04\n",
      "Epoch 12/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 188ms/step - accuracy: 0.9399 - loss: 0.2825 - val_accuracy: 0.9753 - val_loss: 0.1575 - learning_rate: 6.0443e-05\n",
      "Epoch 13/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 184ms/step - accuracy: 0.9426 - loss: 0.2723 - val_accuracy: 0.9728 - val_loss: 0.1659 - learning_rate: 3.2122e-05\n",
      "Epoch 14/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 189ms/step - accuracy: 0.9425 - loss: 0.2714 - val_accuracy: 0.9787 - val_loss: 0.1484 - learning_rate: 1.5054e-05\n",
      "Epoch 15/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 193ms/step - accuracy: 0.9430 - loss: 0.2684 - val_accuracy: 0.9763 - val_loss: 0.1536 - learning_rate: 6.1176e-06\n",
      "Epoch 16/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 196ms/step - accuracy: 0.9401 - loss: 0.2766 - val_accuracy: 0.9775 - val_loss: 0.1499 - learning_rate: 2.1140e-06\n",
      "Epoch 17/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9409 - loss: 0.2726  \n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 3.035484326119331e-07.\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 189ms/step - accuracy: 0.9409 - loss: 0.2726 - val_accuracy: 0.9779 - val_loss: 0.1505 - learning_rate: 3.0355e-07\n",
      "Epoch 18/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 187ms/step - accuracy: 0.9416 - loss: 0.2753 - val_accuracy: 0.9763 - val_loss: 0.1532 - learning_rate: 7.0473e-08\n",
      "Epoch 19/25\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 183ms/step - accuracy: 0.9416 - loss: 0.2703 - val_accuracy: 0.9769 - val_loss: 0.1523 - learning_rate: 1.2782e-08\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 0.14836, Accuracy: 97.87%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'h5py' has no attribute 'File'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 128\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_advanced.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_advanced.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:33\u001b[0m, in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`save_model()` using h5 format requires h5py. Could not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimport h5py.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m     )\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# If file exists and should not be overwritten.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filepath):\n\u001b[0;32m     36\u001b[0m         proceed \u001b[38;5;241m=\u001b[39m io_utils\u001b[38;5;241m.\u001b[39mask_to_proceed_with_overwrite(filepath)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'h5py' has no attribute 'File'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Add, Input, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "# Model Parameters\n",
    "pixel_width = 28\n",
    "pixel_height = 28\n",
    "input_shape = (pixel_width, pixel_height, 1)\n",
    "num_of_classes = 10\n",
    "batch_size = 64\n",
    "epochs = 25  # Increased epochs\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.3  # Reduced dropout slightly\n",
    "l2_reg = 0.0001 # reduced l2 slightly.\n",
    "\n",
    "# Load MNIST Dataset\n",
    "(features_train, labels_train), (features_test, labels_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and Normalize Data\n",
    "features_train = features_train.reshape(-1, pixel_width, pixel_height, 1).astype(\"float32\") / 255.0\n",
    "features_test = features_test.reshape(-1, pixel_width, pixel_height, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_train = keras.utils.to_categorical(labels_train, num_of_classes)\n",
    "labels_test = keras.utils.to_categorical(labels_test, num_of_classes)\n",
    "\n",
    "def elastic_transform_augmentation(images):\n",
    "    \"\"\"Applies elastic transformations using imgaug.\"\"\"\n",
    "    # Ensure images are a numpy array\n",
    "    images = np.array(images)\n",
    "\n",
    "    # Remove the channel dimension to make it (N, 28, 28)\n",
    "    images = np.squeeze(images, axis=-1)\n",
    "\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.ElasticTransformation(alpha=(25, 50), sigma=5.0)  # Adjust alpha and sigma as needed\n",
    "    ])\n",
    "    #imgaug expects a list of images.\n",
    "    augmented_images = seq(images=images)\n",
    "\n",
    "    # Add the channel dimension back\n",
    "    augmented_images = np.expand_dims(np.array(augmented_images), axis=-1)\n",
    "\n",
    "    return augmented_images\n",
    "    \n",
    "# Data Augmentation (Expanded)\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    preprocessing_function=elastic_transform_augmentation,\n",
    ")\n",
    "\n",
    "datagen.fit(features_train)\n",
    "\n",
    "# Residual Block Function\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Enhanced Model with Residual Blocks\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=l2(l2_reg))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = residual_block(x, 32)\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = residual_block(x, 64)\n",
    "x = MaxPooling2D(2)(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(num_of_classes, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Cosine Annealing Learning Rate Schedule\n",
    "def lr_schedule(epoch, lr):\n",
    "    decay_steps = epochs\n",
    "    cosine_decay = 0.5 * (1 + np.cos(np.pi * epoch / decay_steps))\n",
    "    decayed = (1 - 1e-4) * cosine_decay + 1e-4\n",
    "    return lr * decayed\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Optimizer and Compilation\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# Training with Augmented Data\n",
    "model.fit(datagen.flow(features_train, labels_train, batch_size=batch_size),\n",
    "          validation_data=(features_test, labels_test),\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=[early_stopping, reduce_lr, lr_callback])\n",
    "\n",
    "# Evaluation\n",
    "score = model.evaluate(features_test, labels_test, verbose=0)\n",
    "print(f\"Final Loss: {score[0]:.5f}, Accuracy: {score[1] * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('model_advanced.h5')\n",
    "print(\"Model saved as 'model_advanced.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096d60a-561b-492b-b64d-d76ba2c5a367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
