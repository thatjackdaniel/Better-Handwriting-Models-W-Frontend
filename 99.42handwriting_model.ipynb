{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72901238-0a9c-45ff-b0f0-3aa2eb42e7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\mnist_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\ProgramData\\anaconda3\\envs\\mnist_env\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 72ms/step - accuracy: 0.7729 - loss: 1.1073 - val_accuracy: 0.9775 - val_loss: 0.3783 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 76ms/step - accuracy: 0.9406 - loss: 0.5180 - val_accuracy: 0.9844 - val_loss: 0.3909 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 81ms/step - accuracy: 0.9517 - loss: 0.5102 - val_accuracy: 0.9886 - val_loss: 0.3671 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 83ms/step - accuracy: 0.9556 - loss: 0.4844 - val_accuracy: 0.9890 - val_loss: 0.3455 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 82ms/step - accuracy: 0.9583 - loss: 0.4603 - val_accuracy: 0.9850 - val_loss: 0.3589 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 82ms/step - accuracy: 0.9622 - loss: 0.4381 - val_accuracy: 0.9875 - val_loss: 0.3174 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 84ms/step - accuracy: 0.9652 - loss: 0.3918 - val_accuracy: 0.9910 - val_loss: 0.2900 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 87ms/step - accuracy: 0.9658 - loss: 0.3812 - val_accuracy: 0.9917 - val_loss: 0.2807 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 84ms/step - accuracy: 0.9671 - loss: 0.3647 - val_accuracy: 0.9919 - val_loss: 0.2654 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 83ms/step - accuracy: 0.9677 - loss: 0.3493 - val_accuracy: 0.9871 - val_loss: 0.2807 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Final Loss: 0.26537, Accuracy: 99.19%\n",
      "Model saved as 'model.h5'\n"
     ]
    }
   ],
   "source": [
    "### import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Model Parameters\n",
    "pixel_width = 28\n",
    "pixel_height = 28\n",
    "input_shape = (pixel_width, pixel_height, 1)\n",
    "num_of_classes = 10\n",
    "batch_size = 32  # Lower batch size to reduce memory usage\n",
    "epochs = 10  # Reduce epochs to avoid long training times\n",
    "learning_rate = 0.001  # Base learning rate\n",
    "dropout_rate = 0.4  # Adjusted dropout rate to prevent overfitting\n",
    "\n",
    "# Load MNIST Dataset\n",
    "(features_train, labels_train), (features_test, labels_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and Normalize Data\n",
    "features_train = features_train.reshape(-1, pixel_width, pixel_height, 1).astype(\"float32\") / 255.0\n",
    "features_test = features_test.reshape(-1, pixel_width, pixel_height, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_train = keras.utils.to_categorical(labels_train, num_of_classes)\n",
    "labels_test = keras.utils.to_categorical(labels_test, num_of_classes)\n",
    "\n",
    "# 🏆 **Simplified Data Augmentation**\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # Slightly smaller rotation range\n",
    "    zoom_range=0.1,  # Reduced zoom range\n",
    "    width_shift_range=0.1,  # Slightly smaller horizontal shift\n",
    "    height_shift_range=0.1,  # Slightly smaller vertical shift\n",
    "    shear_range=0.05,  # Reduced shear range\n",
    ")\n",
    "datagen.fit(features_train)\n",
    "\n",
    "# 🧠 **Deep CNN Model with Regularization**\n",
    "model = Sequential()\n",
    "\n",
    "# First Conv Block\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Second Conv Block\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.0005)))\n",
    "model.add(Dropout(dropout_rate))  # Apply dropout in the FC layer\n",
    "model.add(Dense(num_of_classes, activation='softmax'))\n",
    "\n",
    "# **Optimizer with Learning Rate Decay**\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 🧠 **Advanced Callbacks**: Patience for early stopping and reduce LR\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7, verbose=1)\n",
    "\n",
    "# 🚀 **Train Model with Augmented Data**\n",
    "model.fit(datagen.flow(features_train, labels_train, batch_size=batch_size),\n",
    "          validation_data=(features_test, labels_test),\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# 🎯 **Evaluate Model**\n",
    "score = model.evaluate(features_test, labels_test, verbose=0)\n",
    "print(f\"🔥 Final Loss: {score[0]:.5f}, Accuracy: {score[1] * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('model.h5')  # Save model to a file\n",
    "print(\"Model saved as 'model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a35ec2-5a63-4b41-af8d-b3f61ffadd00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mnist_env]",
   "language": "python",
   "name": "conda-env-mnist_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
